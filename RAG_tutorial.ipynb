{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c09ff2f-284e-4cdc-b53a-1fc4712ce4db",
   "metadata": {},
   "source": [
    "### Introduction to Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this section, we'll learn how Retrieval Augmented Generation (RAG) works. \n",
    "\n",
    "RAG combines the power of large language models with a retrieval mechanism to provide more accurate and contextually relevant responses. It enhances the capabilities of LLMs by incorporating external knowledge bases, allowing the model to pull in precise information as needed.\n",
    "\n",
    "We'll take the following steps:\n",
    "\n",
    "1. **Set up the LLM**: Just like in previous tutorial, we'll start by setting up the language model.\n",
    "2. **Ask a Question to the LLM**: We'll pose a question to the LLM and observe how it responds using its internal knowledge.\n",
    "3. **Set up the Knowledge Base**: We'll establish a knowledge base (as shown in the video) that the LLM can use to find relevant information.\n",
    "4. **Retrieve Information**: We'll use the knowledge base to retrieve relevant information to answer questions more accurately.\n",
    "5. **Evaluate Responses**: We'll use a small dataset to evaluate RAG responses using the metrics provided by the RAGas library\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05471ac5-c179-4481-a66e-b9e426f5384a",
   "metadata": {},
   "source": [
    "### Step 1: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run only once and then restart kernel\n",
    "%pip install -qU boto3 awscli botocore\n",
    "%pip install ragas==0.1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132b371-ed95-45dd-ba2b-ac252f25a65f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import pprint as pp\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.client import Config\n",
    "\n",
    "from datasets import Dataset \n",
    "from ragas.metrics import *\n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from langchain.embeddings import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a606f-c37f-4607-842d-3fd5948cfcda",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Create Bedrock Clients\n",
    "\n",
    "Retrieval Augmented Generation (RAG) relies on a vector database to store context vectors and retrieve the most relevant ones when a query is made. Amazon Bedrock offers an integrated tool called Knowledge Bases, which simplifies this process. \n",
    "\n",
    "Knowledge Bases for Amazon Bedrock is a fully managed capability that supports the entire RAG workflow, from data ingestion to retrieval and prompt augmentation. This eliminates the need for custom integrations with data sources and complex data flow management. Additionally, you can ask questions and summarize data from a single document without needing to set up a vector database.\n",
    "\n",
    "You can learn more about Bedrock Knowledge Bases [here](https://aws.amazon.com/bedrock/knowledge-bases).\n",
    "\n",
    "In this step, we'll create Bedrock Knowledge Base clientsâ€”instances that enable us to access and utilize Knowledge Bases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8befcb-f5f9-4e90-8151-e8e0c8eda55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create boto3 session\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Create bedrock agent clients\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}, region_name=region_name)\n",
    "chat_client = boto3.client(service_name='bedrock-runtime')\n",
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078abf9-a357-4c43-9d7f-3211a8645755",
   "metadata": {},
   "source": [
    "### Step 3: Choose your LLM\n",
    "\n",
    "Bedrock offers a wide selection of LLMs to choose from. \n",
    "\n",
    "Here are the available options: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html\n",
    "\n",
    "Let's start by testing Claude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a13443-dbb4-4e6d-a1d5-fbabbad0c8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will be using Anthropic Claude 3 Haiku throughout the notebook\n",
    "model_id = \"anthropic.claude-v2\" \n",
    "region_id = boto3_session.region_name #region is required for running the model\n",
    "model_arn = f'arn:aws:bedrock:{region_id}::foundation-model/{model_id}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52276f-bb2f-4c8b-9270-12c325a59cdd",
   "metadata": {},
   "source": [
    "### Step 4: Test your LLM\n",
    "We'll use the function below to test our LLM. This function takes an input prompt and returns the answer. It has already been used in the previous tutorial.\n",
    "In this section, we're asking the model about some iPhone 15 specs to help us make a better decision. Let's see what the model says.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f724d-39c5-4498-b064-56833a40f2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_answer_generator(question_with_prompt):\n",
    "    \"\"\"\n",
    "    This function is used to invoke Amazon Bedrock using the finalized prompt that was created by the prompt_finder(question)\n",
    "    function.\n",
    "    :param question_with_prompt: This is the finalized prompt that includes semantically similar prompts, chat history,\n",
    "    and the users question all in a proper multi-shot format.\n",
    "    :return: The final answer to the users question.\n",
    "    \"\"\"\n",
    "    # body of data with parameters that is passed into the bedrock invoke model request\n",
    "    # TODO: TUNE THESE PARAMETERS AS YOU SEE FIT\n",
    "    body = json.dumps({\"prompt\": question_with_prompt,\n",
    "                       \"max_tokens_to_sample\": 8191,\n",
    "                       \"temperature\": 0,\n",
    "                       \"top_k\": 250,\n",
    "                       \"top_p\": 0.5,\n",
    "                       \"stop_sequences\": []\n",
    "                       })\n",
    "    contentType = 'application/json'\n",
    "    # Invoking the bedrock model with your specifications\n",
    "    response = chat_client.invoke_model(body=body,\n",
    "                                    modelId=model_id,\n",
    "                                    contentType=contentType)\n",
    "    # the body of the response that was generated\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    # retrieving the specific completion field, where you answer will be\n",
    "    answer = response_body.get('completion')\n",
    "    # returning the answer as a final result, which ultimately gets returned to the end user\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e70a6-b578-417a-a791-90930230b397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prompting the LLM\n",
    "messages = '''Human: is the latest iphone 15 splash resistant?\n",
    "Assistant:'''\n",
    "print(llm_answer_generator(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34dea03-daa1-4431-b6a1-4a9a2677b18a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 5: Create a Knowledge Base in the UI\n",
    "\n",
    "As noted before, the model currently doesn't have updates about the latest iPhone. Utilizing Bedrock Knowledge Bases (KBs) allows us to ingest relevant information and answer questions effectively.\n",
    "- *Follow the video and upload the folder data/kb_data to S3 as shown in the video to consume all the knowlege sources required for this video*\n",
    "\n",
    "- Knowledge Bases for Amazon Bedrock offer fully managed capabilities for implementing Retrieval Augmented Generation (RAG) workflows. This includes ingestion, retrieval, and prompt augmentation, all without the need for custom integrations to data sources or management of data flows. Alternatively, you can also ask questions and summarize data from a single document without setting up a vector database.\n",
    "- KBs help create a repository of structured information that the model can retrieve and use to generate responses.\n",
    "- Directions for creating and managing Knowledge Bases are provided in the videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e7c2d-774f-43e7-b42f-59230f769ee5",
   "metadata": {},
   "source": [
    "### Step 6: Create Bedrock Gaurdrails in the UI \n",
    "Bedrock Gaurdrails help establish safety controls and guidelines for the generative AI applications, ensuring they adhere to ethical and policy-based standards.\n",
    "Directions for creating and managing gaurdrails are provided in the videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20d5e6-5a60-43b6-a93a-349ce795a2a9",
   "metadata": {},
   "source": [
    "### Step 7: Test the Knowledge Base Using retrieve_and_generate() API\n",
    "`retrieve_and_generate` in Bedrock combines these configurations to effectively fetch and integrate relevant knowledge from a Knowledge Base, using it to generate informed responses tailored to specific queries or prompts.\n",
    "\n",
    "Here's an explanation of the key parameters used in the `retrieve_and_generate` function:\n",
    "\n",
    "- **query**: This parameter takes in the input text or query for which you want to retrieve information.\n",
    "  \n",
    "- **kb_id**: Specifies the ID of the Knowledge Base (KB) that you've created or want to use for retrieving relevant information.\n",
    "\n",
    "- **model_arn**: ARN (Amazon Resource Name) of the language model or model configuration that will be used for generating responses based on the retrieved information.\n",
    "\n",
    "- **max_results**: Determines the maximum number of results (top N chunks) that the retrieval process will fetch from the Knowledge Base. This helps in finding the most relevant information related to the query.\n",
    "\n",
    "- **prompt_template**: This parameter defines the template or structure of the prompt that the generation process will use. It specifies how the retrieved information will be integrated into the response generation, ensuring that the generated answers are contextually relevant and coherent.\n",
    "\n",
    "- **retrievalConfiguration**: Specifies the configuration settings for the retrieval process:\n",
    "  - **vectorSearchConfiguration**: Configures how the retrieval system searches for relevant documents based on vector similarities. `numberOfResults` within this configuration determines how many top documents should be retrieved.\n",
    "  \n",
    "- **generationConfiguration**: Defines how the generated response will be structured:\n",
    "  - **promptTemplate**: Specifies the template for generating responses. It ensures that the retrieved information is appropriately used to generate coherent answers to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b93e3-bdb4-42ae-9001-1bd7627061aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_and_generate(query, kb_id, model_arn, max_results):\n",
    "    \"\"\"\n",
    "    Function to perform Retrieval Augmented Generation (RAG) using Bedrock.\n",
    "\n",
    "    Parameters:\n",
    "    - query: Input text or query for which information is to be retrieved and generated.\n",
    "    - kb_id: ID of the Knowledge Base (KB) used for retrieval.\n",
    "    - model_arn: ARN (Amazon Resource Name) of the language model for response generation.\n",
    "    - max_results: Maximum number of top documents to retrieve from the KB.\n",
    "    - prompt_template: Template for generating responses based on retrieved information.\n",
    "\n",
    "    Returns:\n",
    "    - response: Generated response including text output and context citations.\n",
    "    \"\"\"\n",
    "    # Call Bedrock's retrieve_and_generate function with specified configurations\n",
    "    response = bedrock_agent_client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': query  # Input query to retrieve and generate information\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',  # Type of retrieval/generation operation\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the Knowledge Base used\n",
    "            'modelArn': model_arn,  # ARN of the language model used\n",
    "            'retrievalConfiguration': {\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results  # Number of top documents to retrieve\n",
    "                }\n",
    "            },\n",
    "            'generationConfiguration': {\n",
    "                'guardrailConfiguration': {\n",
    "                    'guardrailId': 'gtv1a46m6odg',\n",
    "                    'guardrailVersion': '1'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def print_generation_results(response, print_context=True):\n",
    "    \"\"\"\n",
    "    Function to print the generated text and retrieved context from the response.\n",
    "\n",
    "    Parameters:\n",
    "    - response: Response object containing generated text and context citations.\n",
    "    - print_context: Flag to indicate whether to print retrieved context (default is True).\n",
    "    \"\"\"\n",
    "    generated_text = response['output']['text']  # Extract generated text from response\n",
    "    print('Generated FM response:\\n')\n",
    "    pp.pprint(generated_text)  # Pretty print the generated text\n",
    "    \n",
    "    \n",
    "    citations = response[\"citations\"]  # Extract citations or references from the response\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])  # Extract and append retrieved context from references\n",
    "    if print_context is True:\n",
    "        print('\\n\\n\\nRetrieved Context:\\n')\n",
    "        pp.pprint(contexts)  # Pretty print the retrieved context\n",
    "    return generated_text, contexts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5e8ae-51c4-4528-8345-d3267dc1857e",
   "metadata": {},
   "source": [
    "### Step 8: Test Pipeline End to End\n",
    "#### Your KB should have all the data from data/kb_data folder, if you've not done that, please read step 5 again and upload the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d631a0-b464-4b45-bc7d-dde9af86b521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Define the Knowledge Base ID\n",
    "kb_id = \"QEJY4GZNL3\"\n",
    "\n",
    "#Define the query or question to ask the Knowledge Base\n",
    "query = \"is iphone 15 splash resistant?\"\n",
    "results = retrieve_and_generate(query=query, kb_id=kb_id, model_arn=model_arn, max_results=5)\n",
    "\n",
    "# Print the generated response text and retrieved context (if enabled)\n",
    "print_generation_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2bc79f-bbbd-44a5-8c03-f1b5ef6db6b7",
   "metadata": {},
   "source": [
    "### Step 9: Evaluating the RAG Pipeline\n",
    "In this step, we will assess the performance of our RAG pipeline using a sample dataset. We'll utilize a smaller subset named Wiki-Eval, which includes questions pertaining to various historical events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6709f7-d88a-4139-8c13-32819a5dc507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "wiki_dataset = pd.read_parquet(\"./data/wiki_eval.parquet\")\n",
    "wiki_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873f782-67cf-4ee4-8746-d99399e7e046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_answers = []\n",
    "llm_contexts = []\n",
    "#generate answers using the RAG LLM we setup in the previous steps\n",
    "for question in wiki_dataset[\"question\"]:\n",
    "    answer = retrieve_and_generate(query=question, kb_id=kb_id, model_arn=model_arn, max_results=5)\n",
    "    llm_response, retrieved_context = print_generation_results(answer, print_context=False)\n",
    "    llm_answers.append(llm_response)\n",
    "    llm_contexts.append(retrieved_context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c66227-5fda-4ef6-ace8-28978e01ae9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#update dataset with generated response and context\n",
    "wiki_dataset[\"answer\"] = llm_answers\n",
    "wiki_dataset[\"contexts\"] = llm_contexts\n",
    "#format the dataset for evaluation\n",
    "dict_default = wiki_dataset.to_dict(orient='list')\n",
    "dataset = Dataset.from_dict(dict_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc53002-c7cf-4ea9-b9bf-52d197cbb4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setup LLMs and embeddings for evaluation\n",
    "llm_for_evaluation = BedrockChat(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", client=chat_client)\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=chat_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced77b1-2c39-4d8c-ada8-47aba0f20217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''the below is a list of metrics provided by RAGas, you can read them up here: \n",
    "https://docs.ragas.io/en/latest/concepts/metrics/index.html'''\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import (\n",
    "harmfulness, \n",
    "maliciousness, \n",
    "coherence, \n",
    "correctness, \n",
    "conciseness\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f849f4b-6bb8-47be-b6ee-1f68f6f521ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run evaluation\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[harmfulness,coherence], #you can choose any of the above metrics too\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d4b4a-31e3-406c-8572-34023b69a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see results\n",
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c199c5-5c9c-4236-b4c9-ceaf6dc9beda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
